{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Project 7 - For this project we are going to look at data from a mobile cell phone carrier of clients who were on an old plan and switched to one of the two new alternative plans.  Based off the customers behavior within the data they are going to recommend one of the the companies new plans to the customer.  I will be using random forest, decision tree and logistic regression models/predictions to get an idea of which one is the best predictor of the plan the customer should use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "             calls      minutes     messages       mb_used     is_ultra\n",
      "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
      "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
      "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
      "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
      "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
      "max     244.000000  1632.060000   224.000000  49745.730000     1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "#Load the dataset\n",
    "df = pd.read_csv('/datasets/users_behavior.csv')\n",
    "\n",
    "\n",
    "\n",
    "print(df.head(5))\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into features and target\n",
    "features = df.drop(columns=['is_ultra'])  # Drop the target column\n",
    "target = df['is_ultra']  # Binary target (1 for Ultra, 0 for Smart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training, validation, and test sets (60%-20%-20%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.4, random_state=54321)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=54321)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#For my code I started out by loading the data and importing all the pythons functions I thought I would need.  I then split the dataset into the features and target dataframes. From there I used the train_test_split function to create two different dataframes, one with 60% of the data then one with 40% of the data.  I named the 60% group the training group.  With the 40% group I split it into two more dataframes creating a validation dataframe and a testing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 2}\n",
      "Best Accuracy Score: 0.7791601866251944\n"
     ]
    }
   ],
   "source": [
    "#Train multiple models and tune hyperparameters\n",
    "\n",
    "# Logistic Regression Model with tuning hyperparameter\n",
    "log_model = LogisticRegression(random_state=54321, max_iter=1000)\n",
    "log_model.fit(X_train, y_train)\n",
    "log_preds = log_model.predict(X_valid)\n",
    "log_acc = accuracy_score(y_valid, log_preds)\n",
    "\n",
    "\n",
    "max_depth_values = [3, 5, 10, None] \n",
    "min_samples_split_values = [2, 5, 10]\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    for min_samples_split in min_samples_split_values:\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=12345)\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted_valid = model.predict(X_valid)\n",
    "        score = accuracy_score(y_valid, predicted_valid)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {\"max_depth\": max_depth, \"min_samples_split\": min_samples_split}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Accuracy Score:\", best_score)\n",
    "\n",
    "# Decision Tree Model\n",
    "tree_model = DecisionTreeClassifier(max_depth=4, random_state=54321)\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_preds = tree_model.predict(X_valid)\n",
    "tree_acc = accuracy_score(y_valid, tree_preds)\n",
    "\n",
    "tree_model_2 = DecisionTreeClassifier(max_depth=3, random_state=12345)\n",
    "tree_model_2.fit(X_train, y_train)\n",
    "tree_preds_2 = tree_model_2.predict(X_valid)\n",
    "tree_acc_2 = accuracy_score(y_valid, tree_preds_2)\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=54321)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_preds = rf_model.predict(X_valid)\n",
    "rf_acc = accuracy_score(y_valid, rf_preds)\n",
    "\n",
    "rf_model_2 = RandomForestClassifier(n_estimators=5, max_depth=4, random_state=12345)\n",
    "rf_model_2.fit(X_train, y_train)\n",
    "rf_preds_2 = rf_model_2.predict(X_valid)\n",
    "rf_acc_2 = accuracy_score(y_valid, rf_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6750\n",
      "Logistic Regression Accuracy(2): 0.7792\n",
      "Decision Tree Accuracy: 0.7449\n",
      "Decision Tree Accuracy(2): 0.7652\n",
      "Random Forest Accuracy: 0.7978\n",
      "Random Forest Accuracy(2): 0.7760\n"
     ]
    }
   ],
   "source": [
    "#Compare model performance\n",
    "print(f\"Logistic Regression Accuracy: {log_acc:.4f}\")\n",
    "print(f\"Logistic Regression Accuracy(2): {best_score:.4f}\")\n",
    "print(f\"Decision Tree Accuracy: {tree_acc:.4f}\")\n",
    "print(f\"Decision Tree Accuracy(2): {tree_acc_2:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "print(f\"Random Forest Accuracy(2): {rf_acc_2:.4f}\")\n",
    "\n",
    "#Choose the best model (Random Forest is often the best for classification tasks)\n",
    "best_model = rf_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#From there I created a logistic regression model, a decision tree model and a random forest model.  I then tested all the models for the accuracy and deemed the one with the highest accuracy as the best model since it had the best quality.  According to my models the random forest model had the best accuracy with 79.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8336\n"
     ]
    }
   ],
   "source": [
    "# Test the best model\n",
    "test_preds = best_model.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#I then printed the accuracy score for the best model on the prediciton models onto the test model and got the test model accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       466\n",
      "           1       0.77      0.56      0.65       177\n",
      "\n",
      "    accuracy                           0.83       643\n",
      "   macro avg       0.81      0.75      0.77       643\n",
      "weighted avg       0.83      0.83      0.82       643\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Last I print a classification report for the test model.  Based off the data and models we used the 1st random forest model had the best accuracy for predicting values with the training and validation sets.  We then used that model to predict information on the test model.  The test accuracy from the test model came back even higher than the training models at 0.83.  After that we printed a classification report to get the preicsion, recall and F1 scores for the random forest model based off the test data.  The report showed a strong F1 score of 0.89 meaning the model predicted well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1365,
    "start_time": "2025-01-29T23:58:25.423Z"
   },
   {
    "duration": 291,
    "start_time": "2025-01-29T23:59:05.443Z"
   },
   {
    "duration": 282,
    "start_time": "2025-01-30T00:01:58.881Z"
   },
   {
    "duration": 288,
    "start_time": "2025-01-30T00:08:14.169Z"
   },
   {
    "duration": 285,
    "start_time": "2025-01-30T00:08:33.174Z"
   },
   {
    "duration": 298,
    "start_time": "2025-01-30T00:13:57.266Z"
   },
   {
    "duration": 294,
    "start_time": "2025-01-30T00:14:11.144Z"
   },
   {
    "duration": 1228,
    "start_time": "2025-01-30T13:05:11.480Z"
   },
   {
    "duration": 9,
    "start_time": "2025-01-30T21:27:24.608Z"
   },
   {
    "duration": 863,
    "start_time": "2025-01-30T21:27:30.913Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-30T21:27:33.828Z"
   },
   {
    "duration": 6,
    "start_time": "2025-01-30T21:27:34.464Z"
   },
   {
    "duration": 537,
    "start_time": "2025-01-30T21:27:35.244Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-30T21:27:36.015Z"
   },
   {
    "duration": 301,
    "start_time": "2025-01-30T21:28:01.124Z"
   },
   {
    "duration": 292,
    "start_time": "2025-01-30T21:28:11.592Z"
   },
   {
    "duration": 290,
    "start_time": "2025-01-30T21:28:54.271Z"
   },
   {
    "duration": 293,
    "start_time": "2025-01-30T21:29:02.555Z"
   },
   {
    "duration": 285,
    "start_time": "2025-01-30T21:29:37.195Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-30T21:29:39.691Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-30T21:29:56.391Z"
   },
   {
    "duration": 283,
    "start_time": "2025-01-30T21:30:05.121Z"
   },
   {
    "duration": 3,
    "start_time": "2025-01-30T21:30:06.666Z"
   },
   {
    "duration": 289,
    "start_time": "2025-01-30T21:30:14.400Z"
   },
   {
    "duration": 279,
    "start_time": "2025-01-30T21:30:17.215Z"
   },
   {
    "duration": 291,
    "start_time": "2025-01-30T21:30:32.795Z"
   },
   {
    "duration": 14,
    "start_time": "2025-01-30T21:30:40.395Z"
   },
   {
    "duration": 7,
    "start_time": "2025-01-30T21:30:44.984Z"
   },
   {
    "duration": 4,
    "start_time": "2025-01-30T21:30:47.683Z"
   },
   {
    "duration": 167,
    "start_time": "2025-02-04T01:11:58.468Z"
   },
   {
    "duration": 827,
    "start_time": "2025-02-04T01:12:06.354Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-04T01:12:07.183Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-04T01:12:07.386Z"
   },
   {
    "duration": 88,
    "start_time": "2025-02-04T01:12:08.788Z"
   },
   {
    "duration": 356,
    "start_time": "2025-02-04T01:13:23.713Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-04T01:14:35.202Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
